---
title:  "Evaluating Election Forecasts"
author: "Jake Thompson"
date: 2016-11-08
categories: ["rstats"]
tags: [politics]
---

<!-- BLOGDOWN-HEAD -->
<!-- /BLOGDOWN-HEAD -->


<p>After more than a year and a half, the 2016 presidential election is finally over, with Donald Trump <a href="http://www.nytimes.com/elections/results/president">projected to win</a>. This is in contrast to many of the election forecasts, which almost unanimously predicted a <a href="http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html">victory for Hillary Clinton</a>. Now that the results are in, we can finally answer the question of who did the best (or least worst?) job of forecasting the election.</p>
<p>There are several ways we could look at this question, but for the purpose of this analysis we’ll focus on two. The first is prediciton accuracy: how close were a model’s probabilities of winning each state to what actually happened? The second is model accuracy: if we assume that the model estimated probabilities were correct, how likely is it that we would see the outcome that was observed?</p>
<p>Now, let’s see how some different forecasts performed.</p>
<div id="the-forecasts" class="section level2">
<h2>The Forecasts</h2>
<p>For this analysis I’m going to focus on some of the more popular forecasts from this election cycle: the <a href="http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html">New York Times</a>, <a href="http://projects.fivethirtyeight.com/2016-election-forecast/?ex_cid=rrpromo">FiveThirtyEight</a>, <a href="http://elections.huffingtonpost.com/2016/forecast/president">Huffington Post</a>, <a href="http://predictwise.com/">PredictWise</a>, the <a href="http://election.princeton.edu/2012/09/29/the-short-term-presidential-predictor/">Princeton Election Consortium</a>, <a href="https://twitter.com/PollSavvy">Poll Savvy</a>, <a href="http://elections.dailykos.com/app/elections/2016">Daily Kos</a>, <a href="http://www.thecrosstab.com/2016/11/07/final-2016-forecast/">Crosstab</a>, and <a href="http://www.slate.com/features/pkremp_forecast/report.html">Slate (Pierre-Antoine Kremp)</a>. Additionally, I’ll include the 2012 electoral map as a baseline. A special thanks to <a href="http://dannypage.github.io/">Danny Page</a> who compiled all of the <a href="https://docs.google.com/spreadsheets/d/1WZdg3fcvK_J-XRtN8WpEb9nRB9nF-gi6DRDYINku_PU/edit#gid=0">data</a>, and whose analysis on the different forecasts can be found <a href="https://twitter.com/dannypage">here</a>. As always, the code and data for this post can be found on <a href="https://github.com/wjakethompson/wjakethompson.github.io">my github</a>.</p>
</div>
<div id="prediction-accuracy" class="section level2">
<h2>Prediction Accuracy</h2>
<p>For the first part of the analysis, we’ll look at how close the predictions were in each state for each model. To do this, we’ll use the <a href="https://en.wikipedia.org/wiki/Brier_score">Brier Score</a>, which is a measure of how close predicted probabilities were to an observed event. The formula for the Brier Score is given as</p>
<span class="math display" id="eq:brier">\[\begin{equation}
BS = \frac{1}{N} \displaystyle\sum_{t=1}^{N}(f_t-o_t)^2
\tag{1}
\end{equation}\]</span>
<p>where “N” is the total number of predictions, “f” is the predicted probability of the event, and “o” is the observed outcome. Thus, a perfect prediction (e.g., a probability of 1.0 and the event actually occuring, or a probability of 0.0 and the event not occuring) would result in a Brier Score of 0, and the worst possible score is 1. For example if a model predicted Hillary Clinton had an 87 percent chance of winning a state, and she ended up winning that state, that part of the Brier score would be calculated as <span class="math inline">\((0.87 - 1)^2 = 0.0169\)</span>. Because the probability is close to 1, and the event happened, the Brier Score is low, indicating high prediction accuracy. In contrast, if Clinton were to lose that state, the Brier score would be <span class="math inline">\((0.87 - 0)^2 = 0.7569\)</span>, a relatively large number indicating a bad prediction. For any given model then, we can add up these prediction errors for each state, and then divide by the total number of predictions to get an overall Brier Score for each model:</p>
<table>
<thead>
<tr class="header">
<th align="left">Forecast</th>
<th align="left">Brier Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FiveThirtyEight</td>
<td align="left">0.066</td>
</tr>
<tr class="even">
<td align="left">Crosstab</td>
<td align="left">0.070</td>
</tr>
<tr class="odd">
<td align="left">Slate</td>
<td align="left">0.076</td>
</tr>
<tr class="even">
<td align="left">PredictWise</td>
<td align="left">0.076</td>
</tr>
<tr class="odd">
<td align="left">New York Times</td>
<td align="left">0.076</td>
</tr>
<tr class="even">
<td align="left">Princeton</td>
<td align="left">0.077</td>
</tr>
<tr class="odd">
<td align="left">Poll Savvy</td>
<td align="left">0.080</td>
</tr>
<tr class="even">
<td align="left">Daily Kos</td>
<td align="left">0.080</td>
</tr>
<tr class="odd">
<td align="left">Huffington Post</td>
<td align="left">0.090</td>
</tr>
<tr class="even">
<td align="left">2012 Map</td>
<td align="left">0.125</td>
</tr>
</tbody>
</table>
<p>Using this method, all models out perform the 2012 map, as we would expect. Nate Silver and FiveThirtyEight come out on top, with the lowest average prediction error. However, we can also look at a weighted Brier Score. Maybe we care more about an error if there were a large number of electoral votes were at stake. For example, predicting Florida incorrectly has more implications for who wins the presidency than predicting North Dakota incorrectly. We can add a weight to the Brier Score formula by multiplying the electoral votes by the error.</p>
<span class="math display" id="eq:brier-wt">\[\begin{equation}
BS = BS = \displaystyle\sum_{t=1}^{N}EV_t(f_t-o_t)^2
\tag{2}
\end{equation}\]</span>
<p>For this method, we are just summing all of the weighted prediction errors, rather than taking the average. This means that the Brier score is equivalent to the number of electoral votes that were incorrectly predicted. Thus, a perfect score is still 0, but the worst possible score is 538, if every state were incorrectly predicted with a probability of 0 or 1.</p>
<p>Using the weighted Brier Score, we see similar results.</p>
<table>
<thead>
<tr class="header">
<th align="left">Forecast</th>
<th align="left">Brier Score</th>
<th align="left">Weighted Brier</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FiveThirtyEight</td>
<td align="left">0.066</td>
<td align="left">48.793</td>
</tr>
<tr class="even">
<td align="left">Crosstab</td>
<td align="left">0.070</td>
<td align="left">59.601</td>
</tr>
<tr class="odd">
<td align="left">Princeton</td>
<td align="left">0.077</td>
<td align="left">62.813</td>
</tr>
<tr class="even">
<td align="left">New York Times</td>
<td align="left">0.076</td>
<td align="left">64.643</td>
</tr>
<tr class="odd">
<td align="left">Poll Savvy</td>
<td align="left">0.080</td>
<td align="left">64.668</td>
</tr>
<tr class="even">
<td align="left">Slate</td>
<td align="left">0.076</td>
<td align="left">65.038</td>
</tr>
<tr class="odd">
<td align="left">PredictWise</td>
<td align="left">0.076</td>
<td align="left">68.132</td>
</tr>
<tr class="even">
<td align="left">Daily Kos</td>
<td align="left">0.080</td>
<td align="left">69.283</td>
</tr>
<tr class="odd">
<td align="left">Huffington Post</td>
<td align="left">0.090</td>
<td align="left">80.722</td>
</tr>
<tr class="even">
<td align="left">2012 Map</td>
<td align="left">0.125</td>
<td align="left">100.000</td>
</tr>
</tbody>
</table>
<p>FiveThirtyEight again performs the best, prediting the equivalent of about 49 electoral votes incorrectly, a full 11 electoral votes better than the second best performing model (Crosstab). Compare this to the 2012 map, which would have predicted 100 electoral votes incorrectly. So using either the raw or weighted Brier Score, all models performed better than our baseline of the 2012 map, and FiveThirtyEight performed better than all other models.</p>
</div>
<div id="model-accuracy" class="section level2">
<h2>Model Accuracy</h2>
<p>To get an idea of the model accuracy, we can simulate the election thousands of times for each model, assuming that their estimated probabilities are the true probabilities in each case. For each simulation we will then get a distribution of the number of states each model should expect to predict incorrectly. We will then compare this expected number of incorrect picks to the observed number. A large discrepancy between the expected and observed mean that the model’s probabilites were not consistent with the observed data, indicating poor model accuracy.</p>
<p><img src="#####content/post/2016-11-08-election-forecast-performance_files/figure-html/sim-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In the plot above, the red line indicates the number of states the model actually predicted incorrectly. All of the model incorrectly predicted 5 states, with the exception of Princeton, which incorrectly predicted only 4 states. However, we see that we would expect most of the models to get less states incorrect than that, given their estimated probabilities. For example, we would expect Huffington Post to miss 1.25 states on average, and FiveThirtyEight to miss 5.37 states on average, given their probabilities in each state. So the Huffington Post’s model was not as accurate as FiveThirtyEight’s.</p>
<p>This occurs because Huffington Post had less uncertainty in their model, so there were fewer states that their model thought had a chance to flip, and thus result in an incorrect prediction. FiveThirtyEight had more uncertainty, meaning that on average, their model would expect to get more states wrong, because there was less certainty in who would win each state. As it turns out, this election was a lot more uncertain than people thought, which is why FiveThirtyEight’s expected value is closer to what was observed than other models that had less uncertainty.</p>
<p>We can quantify how wrong a model was by looking at the proportion of simualtions that resulted in more incorrect picks than what was observed (the p-value). This can be thought of as the probability of incorrectly picking the observed number, if your probabilities were correct. Traditionally, p-values less than 0.05 are used to indicate poor model fit. In the graphic, we can see that only FiveThirtyEight, the New York Times, Poll Savvy, and Princeton have p-values over 0.05. This means that we would conclude that the probabilities provided by these models are plausible, given the data we observed, whereas the other probabilities provided by the other models would be rejected.</p>
<p>There is one problem with this simulation that we could address to make our results more accurate. The simulation assumes that the results in each state are indepdent of each other. However, we know this is inaccurate, as many states tend to vote in the same way as other states with similar demographic characteristics. Practically, this means that the distributions in the above graphic aren’t as variable as they should be. The expected value won’t change much, but the distribution of plausible numbers of incorrect picks should get wider. Let’s test that and see if our conclusions are the same.</p>
<p><img src="#####content/post/2016-11-08-election-forecast-performance_files/figure-html/corsim-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As expected, the distributions are much wider once we allow for correlated prediction errors, especially for the models that have more uncertainty in their predictions. Using the correlated errors, Princeton, Crosstab, Poll Savvy, the New York Times, and FiveThirtyEight all have p-values greater than 0.05, indicating that their estimated probabilities fit our observed data.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Nate Silver and FiveThirtyEight received a lot of <a href="http://www.huffingtonpost.com/entry/nate-silver-election-forecast_us_581e1c33e4b0d9ce6fbc6f7f?ncid=engmodushpmg00000004">criticism</a> for being bullish on Trump, but it looks like FiveThirtyEight gets to have the last laugh. Their model outperformed all of the other election forecasts that I analyzed: it had the best prediciton accuracy as measured by the raw and unweighted Brier Score, and it also had the best model accuracy, with simulations using their probabilities more closely matching the observed data than any other model. In an incredibly messing election, Nate Silver was the real winner.</p>
<p><details><summary>Session info</summary></p>
<pre class="r"><code>devtools::session_info()
#&gt; Session info -------------------------------------------------------------
#&gt;  setting  value                       
#&gt;  version  R version 3.4.0 (2017-04-21)
#&gt;  system   x86_64, darwin15.6.0        
#&gt;  ui       X11                         
#&gt;  language (EN)                        
#&gt;  collate  en_US.UTF-8                 
#&gt;  tz       America/Chicago             
#&gt;  date     2017-05-25
#&gt; Packages -----------------------------------------------------------------
#&gt;  package    * version    date       source                            
#&gt;  assertthat   0.2.0      2017-04-11 CRAN (R 3.4.0)                    
#&gt;  backports    1.0.5      2017-01-18 CRAN (R 3.4.0)                    
#&gt;  base       * 3.4.0      2017-04-21 local                             
#&gt;  bindr        0.1        2016-11-13 cran (@0.1)                       
#&gt;  bindrcpp   * 0.1        2016-12-11 cran (@0.1)                       
#&gt;  blogdown     0.0.41     2017-05-23 Github (rstudio/blogdown@a367835) 
#&gt;  bookdown     0.4        2017-05-23 Github (rstudio/bookdown@b9f0e40) 
#&gt;  cellranger   1.1.0      2016-07-27 CRAN (R 3.4.0)                    
#&gt;  codetools    0.2-15     2016-10-05 CRAN (R 3.4.0)                    
#&gt;  colorspace   1.3-2      2016-12-14 CRAN (R 3.4.0)                    
#&gt;  compiler     3.4.0      2017-04-21 local                             
#&gt;  datasets   * 3.4.0      2017-04-21 local                             
#&gt;  devtools     1.13.1     2017-05-13 CRAN (R 3.4.0)                    
#&gt;  digest       0.6.12     2017-01-27 CRAN (R 3.4.0)                    
#&gt;  dplyr      * 0.6.0      2017-05-23 Github (tidyverse/dplyr@c7ca374)  
#&gt;  evaluate     0.10       2016-10-11 CRAN (R 3.4.0)                    
#&gt;  ggplot2    * 2.2.1      2016-12-30 CRAN (R 3.4.0)                    
#&gt;  glue         1.0.0      2017-04-17 cran (@1.0.0)                     
#&gt;  graphics   * 3.4.0      2017-04-21 local                             
#&gt;  grDevices  * 3.4.0      2017-04-21 local                             
#&gt;  grid         3.4.0      2017-04-21 local                             
#&gt;  gtable       0.2.0      2016-02-26 CRAN (R 3.4.0)                    
#&gt;  highr        0.6        2016-05-09 CRAN (R 3.4.0)                    
#&gt;  htmltools    0.3.6      2017-04-28 CRAN (R 3.4.0)                    
#&gt;  knitr      * 1.16       2017-05-18 CRAN (R 3.4.0)                    
#&gt;  labeling     0.3        2014-08-23 CRAN (R 3.4.0)                    
#&gt;  lazyeval     0.2.0      2016-06-12 CRAN (R 3.4.0)                    
#&gt;  magrittr     1.5        2014-11-22 CRAN (R 3.4.0)                    
#&gt;  MASS       * 7.3-47     2017-02-26 CRAN (R 3.4.0)                    
#&gt;  memoise      1.1.0      2017-04-21 CRAN (R 3.4.0)                    
#&gt;  methods      3.4.0      2017-04-21 local                             
#&gt;  munsell      0.4.3      2016-02-13 CRAN (R 3.4.0)                    
#&gt;  pkgconfig    2.0.1      2017-03-21 cran (@2.0.1)                     
#&gt;  plyr         1.8.4      2016-06-08 CRAN (R 3.4.0)                    
#&gt;  purrr      * 0.2.2.2    2017-05-11 CRAN (R 3.4.0)                    
#&gt;  R6           2.2.1      2017-05-10 cran (@2.2.1)                     
#&gt;  Rcpp         0.12.11    2017-05-22 CRAN (R 3.4.0)                    
#&gt;  readxl     * 1.0.0      2017-04-18 CRAN (R 3.4.0)                    
#&gt;  rlang        0.1.1.9000 2017-05-23 Github (hadley/rlang@7c2f7e8)     
#&gt;  rmarkdown    1.5.9000   2017-05-23 Github (rstudio/rmarkdown@54bf8fc)
#&gt;  rprojroot    1.2        2017-01-16 CRAN (R 3.4.0)                    
#&gt;  scales       0.4.1      2016-11-09 CRAN (R 3.4.0)                    
#&gt;  stats      * 3.4.0      2017-04-21 local                             
#&gt;  stringi      1.1.5      2017-04-07 CRAN (R 3.4.0)                    
#&gt;  stringr      1.2.0      2017-02-18 CRAN (R 3.4.0)                    
#&gt;  tibble       1.3.1      2017-05-17 CRAN (R 3.4.0)                    
#&gt;  tools        3.4.0      2017-04-21 local                             
#&gt;  utils      * 3.4.0      2017-04-21 local                             
#&gt;  withr        1.0.2      2016-06-20 CRAN (R 3.4.0)                    
#&gt;  yaml         2.1.14     2016-11-12 CRAN (R 3.4.0)</code></pre>
<p></details></p>
</div>
